---
title: "Causes of Varying Death Rates in the U.S."
subtitle: "W203 Lab 2"
author: "Jenny Conde, Spencer Hong, Kevin Lu, Spencer Song"
output: pdf_document
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(stargazer)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(dplyr)
library(patchwork)
library(olsrr)
library(magick)
library(knitr)
library(ggcorrplot)
require(gridExtra)
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
df <- read_csv('FinalDF.csv')
df_pre_drop <- df
df <- df[-c(2, 9, 12), ] # remove Alaska, Hawaii, D.C.
```

# Introduction

<!-- Your introduction should present a research question and explain the concept that you're attempting to measure and how it will be operationalized. This section should pave the way for the body of the report, preparing the reader to understand why the models are constructed the way that they are. It is not enough to simply say "We are looking for policies that help against COVID."  Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propelling the narrative forward. This is also good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data.

(From Rubric) Is the introduction clear? Is the research question specific and well defined? Does the introduction motivate a specific concept to be measured and explain how it will be operationalized. Does it do a good job of preparing the reader to understand the model specifications?

TODO: jenny conde
Driving question: What causes higher death rates in some states versus others?
  What factors impact ___
Variable categories: Health, policy, socioeconomic, other -->

## Background and Overview

The COVID-19 pandemic has overturned nearly all aspects of normal life over the past year. The U.S. has particularly suffered, with 550,000 COVID-19 deaths, more than any other nation in the world [(NY Times)](https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html). However, not all areas of the U.S. have been equally affected. While some states have seen relatively few cases and deaths, others have been significantly impacted, seeing overwhelmed healthcare systems and lockdowns. This motivates our primary research question: What causes higher fatality rates from COVID-19 in some states versus others? What are the primary factors driving higher fatality rates across states?

To answer this question, we analyze the cumulative COVID-19 fatality rate from January 22, 2020, through March 31, 2021. We calculate the fatality rate as the number of deaths per positive cases. We operationalize several categories of factors that likely have a causal relationship with fatality rate based on our research from media articles, publications, and government sources. These categories include pre-existing health conditions, enacted policy, and socioeconomic characteristics. We additionally include number of ICU beds per 10,000 people to represent the capacity of healthcare systems in each state.

With this background knowledge about factors making people more prone to catching a severe case of COVID-19, we can perform a causal analysis using the one-equation structural model to determine how prevalent these factors are at a state-level. Our causal analysis consists of three models. Our first model contains the variables we believe have the largest causal impact on death rate. Our second model builds on this to include more of our dataset, and finally, our third model includes all data we collected.

## Data Collection and Cleaning

<!-- TODO: spencer song

- explain how we define "death rate" (i.e. deaths/cases)--why is this operationalization more relevant than alternatives (ex. deaths per capita) -- Jenny explains this in the intro a bit but this operationalization probably merits further explanation-->

We collected data for each of our primary categories: pre-existing health conditions, implemented policy, and socioeconomic status. Our operationalized pre-existing health conditions were largely informed by the Center for Disease Control's list of medical conditions that make adults more likely to get severely ill from COVID-19 [(CDC)](https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-with-medical-conditions.html). We focused our policy data on mask requirements and stay at home orders. Individuals' socioeconomic status has also been known to make them more vulnerable to COVID-19, so we include features like unemployment, income, and crowding [(McKinsey)](https://www.mckinsey.com/featured-insights/coronavirus-leading-through-the-crisis/charting-the-path-to-the-next-normal/socioeconomic-vulnerability-increases-the-risk-of-dying-from-covid-19#:~:text=Socioeconomic%20vulnerability%20increases%20the%20risk%20of%20dying%20from%20COVID%2D19,-COVID%2D19%20Inequality&text=September%2010%2C%202020%20People%20who,coronavirus%20than%20the%20general%20population.). Finally, public officials have stressed ICU capacity as an important factor in handling large numbers of severe COVID-19 cases, so we include the number of ICU beds per 10,000 people as an additional feature.

Much of this data was collected from a variety of sources, but we were able to join on State Federal Information Processing System (FIPS) Codes. Our data was primarily on a county level, so we aggregated our data to a state level and converted the data to percentage rates to use for our regression. We chose to collect data on a county level because more granular data is likely more precise. Additionally, this opens the opportunity for future analysis at a county level. We made the choice to define "Fatality Rate" as the total number of deaths divided by total number of cases for each state from January 22, 2020 through March 31, 2021. This normalization allows us to focus on why people are dying from COVID-19 at higher rates rather than how COVID-19 spreads in states.

We also made the choice to omit Hawaii, Alaska, and District of Columbia from our states, as we found that these states were outliers in our socioeconomic analysis and policy factors. District of Columbia is primarily urban, so the demographics, multi-unit housing percentages, and crowding were all drastically different from the rest of our data points, causing nonlinearity with our regressions due to our small dataset. Similarly, Hawaii and Alaska are not a part of the contiguous United States, so their COVID-19 management was much more travel/flight dependent, whereas the rest of the United States was openly exposed to travel. Because these two states are isolated from the rest of the U.S., they can more effectively regulate travel into and out of the state. Finally, Hawaii and Alaska have significantly different demographic characteristics---a high number of minorities and low population density, respectively. These states had many different factors that would impact the fatality of their COVID-19 cases, so for the purpose of our question, we are bounding our dataset to the lower 48 states.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = 'Outliers in socioeconomic data often included Alaska, Hawaii, and the District of Columbia, which is one reason for dropping these observations from our dataset.', fig.pos='!h', out.extra='', fig.align="center", out.width = "75%"}
minority_quantiles <- quantile(df_pre_drop$Minority_Percentage, probs = c(0.25, 0.75))
multi_unit_quantiles <- quantile(df_pre_drop$Multi_Unit_Perc, probs = c(0.25, 0.75))
crowding_quantiles <- quantile(df_pre_drop$Crowding_Perc, probs = c(0.25, 0.75))

minority_IQR <- minority_quantiles[2] - minority_quantiles[1]
multi_unit_IQR <- multi_unit_quantiles[2] - multi_unit_quantiles[1]
crowding_IQR <- crowding_quantiles[2] - crowding_quantiles[1]

df_pre_drop <- df_pre_drop %>% mutate(
  minority_outliers = ifelse(Minority_Percentage < minority_quantiles[1]-1.5*minority_IQR | Minority_Percentage > minority_quantiles[2]+1.5*minority_IQR, state, ""),
  # minority_outliers = ifelse(state == 'Hawaii', 'Hawaii', ''),
  # multi_unit_outliers = ifelse(state == 'District of Columbia', 'District of Columbia', ''),
  multi_unit_outliers = ifelse(Multi_Unit_Perc < multi_unit_quantiles[1]-1.5*multi_unit_IQR | Multi_Unit_Perc > multi_unit_quantiles[2]+1.5*multi_unit_IQR, state, ""),
  crowding_outliers = ifelse(Crowding_Perc < crowding_quantiles[1]-1.5*crowding_IQR | Crowding_Perc > crowding_quantiles[2]+1.5*crowding_IQR, state, ""),
)

label_size <- 3.6

Minority_plot <- ggplot(df_pre_drop, aes(Minority_Percentage, y = 0)) +
  geom_boxplot() + 
  geom_text(size = label_size, aes(label = minority_outliers), nudge_y = 0.15) +
  labs(y = '', x = '% Minorities', title = 'Boxplots for Various Demographic Features and their Outliers') +
  theme_minimal() + theme(axis.text.y = element_blank(), axis.ticks = element_blank())
Multi_Unit_Plot <- ggplot(df_pre_drop, aes(Multi_Unit_Perc, y = 0)) +
  geom_boxplot() + 
  geom_text(size = label_size, aes(label = multi_unit_outliers), nudge_y = 0.15, nudge_x = -0.035) +
  labs(y = '', x = '% Adults in Multi-Unit Dwellings') +
  theme_minimal() + theme(axis.text.y = element_blank(), axis.ticks = element_blank())
Crowding_Plot <- ggplot(df_pre_drop, aes(Crowding_Perc, y = 0)) +
  geom_boxplot() + 
  geom_text(size = label_size, aes(label = crowding_outliers), nudge_y = 0.15) +
  labs(y = '', x = '% Crowding') +
  theme_minimal() + theme(axis.text.y = element_blank(), axis.ticks = element_blank())
combined_plot <- (Minority_plot / Multi_Unit_Plot / Crowding_Plot)
combined_plot
```


These are the variables we have collected and considered prior to performing exploratory data analysis.

1. \textbf{Response Variable}
\vspace{-5pt}

> Fatality Rate [(USA Facts)](https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/): calculated as the ratio of cumulative deaths to cumulative cases

2. \textbf{Health Variables}
\vspace{-5pt}

> 1. % Adults with Diabetes [(CDC)](https://gis.cdc.gov/grasp/diabetes/DiabetesAtlas.html#)
\vspace{-5pt}

> 2. % Deaths from Stroke [(CDC)](https://nccd.cdc.gov/DHDSPAtlas/?state=County)
\vspace{-5pt}

> 3. % Deaths from Heart Disease [(CDC)](https://nccd.cdc.gov/DHDSPAtlas/?state=County)
\vspace{-5pt}

> 4. % Adults who Smoke [(CHR)](https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation)
\vspace{-5pt}

> 5. % Deaths from Chronic Respiratory Disease [(GHDx)](http://ghdx.healthdata.org/record/ihme-data/united-states-chronic-respiratory-disease-mortality-rates-county-1980-2014)
\vspace{-5pt}

> 6. % Deaths from Asthma [(GHDx)](http://ghdx.healthdata.org/record/ihme-data/united-states-chronic-respiratory-disease-mortality-rates-county-1980-2014)
\vspace{-5pt}

> 7. % Adults with Obesity [(CHR)](https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation)

3. \textbf{Policy Variables}
\vspace{-5pt}

> 1. Mask Ratio [(W203)](https://docs.google.com/spreadsheets/d/1zu9qEWI8PsOI_i8nI_S29HDGHlIp2lfVMsGxpQ5tvAQ/edit#gid=1348247662): calculated as the proportion of days each state had a mask mandate between April 8, 2020, and April 4, 2021
\vspace{-5pt}

> 2. Stay at Home Ratio [(W203)](https://docs.google.com/spreadsheets/d/1zu9qEWI8PsOI_i8nI_S29HDGHlIp2lfVMsGxpQ5tvAQ/edit#gid=1348247662): calculated as the proportion of days each state had a stay at home mandate between March 19, 2020, and January 25, 2021

4. \textbf{Socioeconomic Variables}
\vspace{-5pt}

> 1. % Unemployed Pre-Pandemic [(CDC)](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)
\vspace{-5pt}

> 2. % Minority [(CDC)](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)
\vspace{-5pt}

> 3. % Crowding [(CDC)](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)
\vspace{-5pt}

> 4. % Adults Living in Multi-Unit Dwellings [(CDC)](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)
\vspace{-5pt}

> 5. % Adults Living in Poverty [(CDC)](https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html)
\vspace{-5pt}

> 6. Median Household Income [(World Population Review)](https://worldpopulationreview.com/state-rankings/median-household-income-by-state)
\vspace{-5pt}

> 7. % Adults Over Age 65 [(CHR)](https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation)
\vspace{-5pt}

5. \textbf{Miscellaneous Variables}
\vspace{-5pt}

> 1. ICU Beds per 10,000 people [(KFF)](https://www.kff.org/other/state-indicator/icu-beds/?currentTimeframe=0&sortModel=%7B%22colId%22:%22Location%22,%22sort%22:%22asc%22%7D)

# Model Building Process

<!--You will next build a set of models to investigate your research question, documenting your decisions. Here are some things to keep in mind during your model building process:

1. *What do you want to measure*? Make sure you identify one key variable (possibly more in rare cases) that will allow you to derive conclusions relevant to your research question, and include this variables in all model specifications.

2. Is your modeling goal one of description or explanation?-

3. What [covariates](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms) help you achieve your modeling goals? What covariates are problematic, either due to *collinearity*, or because they will absorb some of a causal effect you want to measure?

4. What *transformations*, if any, should you apply to each variable? These transformations might reveal linearities in the data, make your results relevant, or help you meet model assumptions.

5. Are your choices supported by exploratory data analysis (*EDA*)? You will likely start with some general EDA to *detect anomalies* (missing values, top-coded variables, etc.). From then on, your EDA should be interspersed with your model building. Use visual tools to *guide* your decisions. You can also leverage statistical *tests* to help assess whether variables, or groups of variables, are improving model fit.
  - remove mask policy?-->

<!-- 1. What do you want to measure?
The team identified fatality rates as the key variable to be measured as we look to understand the relationship between fatality rates and several chosen variables that fall within the health, policy, or socioeconomic realms. Fatality rates are calculated as CHECK THIS!!!!! number of deaths / number of cases in a state over the entire course of our dataset. -->

## Measurement/Response Variable

The team identified fatality rates as the key variable to be measured as we look to understand the relationship between fatality rates and several chosen variables that fall within the health, policy, or socioeconomic categories.

## An Explanatory Analysis
Our model is an explanatory one, as we not only describe the COVID-19 data and trends but also use the one-equation structural model to analyze causal relationships between each chosen explanatory variable and the response variable. We plan on explaining causes in the changes in fatality rates.

<!-- 3. What [covariates](https://en.wikipedia.org/wiki/Dependent_and_independent_variables#Statistics_synonyms) help you achieve your modeling goals? What covariates are problematic, either due to *collinearity*, or because they will absorb some of a causal effect you want to measure? -->

## Covariate Descriptions and Initial Exploratory Data Analysis

\textbf{Health}: We selected seven different health covariates as listed out above. These variables were chosen because the CDC has claimed that adults with these conditions are more likely to suffer from severe cases of COVID-19 [(CDC)](https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/people-with-medical-conditions.html). However, a few causal relationships exist among these health variables. For example, obesity is a known cause of diabetes, and smoking is known to cause asthma. We additionally found high correlations between these variables. Because of this, we will limit our use of health variables in our models and ensure that no causal relationships exist between remaining variables.

\textbf{Policy}: The covariates for both mask mandate and stay-at-home policy duration are of concern as they both show a positive linear relationship with fatality rates.

\textbf{Socioeconomic}: We collected data for seven socioeconomic covariates that we believe could cause varying COVID-19 fatality rates. Studies have shown that at-risk communities already suffering from poverty and unemployment also are more affected by COVID-19, and COVID-19 has additionally disproportionately impacted communities of color [(CDC)](https://www.cdc.gov/coronavirus/2019-ncov/community/health-equity/race-ethnicity.html). Because of this, we collected data to fully represent states' socioeconomic statuses.  We found that transformations were able to improve the linear relationships for minority percent. Outside of the transformations, we found positive correlations with fatality rates in unemployment rate and minority percentage. Slight positive correlations with poverty percent and household income (which is concerning). We also did not see any correlation with crowding or multi-unit percent which is also counter intuitive since  both considered important metrics by health officials.  We also found high correlations between several of the variables and made the decision to not include ones that showed high collinearity.

\textbf{Miscellaneous}: The covariate for ICU beds per 10,000 appears to be of little concern since it is not highly collinear with any of our other features. 

## Covariate Transformations

<!-- 4. What *transformations*, if any, should you apply to each variable? These transformations might reveal linearities in the data, make your results relevant, or help you meet model assumptions. -->

When performing EDA on our features, we plotted and analyzed scatterplots of fatality rate versus each explanatory variable. We assessed how transformations could make the relationships between our explanatory and response variables more linear and tried various combinations of logarithmic and polynomial transformations. Based on our analysis, we decided to apply no transformation to fatality rate, our response variable, and the subsections below outline the transformations made to the variables within each subset. A sample of our scatterplots are shown in Figure 2. We assessed relationships between explanatory variables and fatality rate and also relationships among the explanatory variables. We conducted similar analyses for all of the variables and the variable categories. 

```{r, echo=FALSE, warning = FALSE, message = FALSE, fig.cap="Scatterplots for a sample of explanatory variables vs Fatality Rate", fig.pos='!h', out.extra='', fig.align="center", out.width = "105%"}
df_figs <- read_csv('FinalDF_nicecols.csv')
df_figs <- df_figs[-c(2, 9, 12), ] # remove Alaska, Hawaii, D.C.

pairs(df_figs[,6:10], pch = 19, lower.panel = NULL)

# Scatter plots per explanatory variable
# mask_scatter <- ggplot(df_figs, aes(x = `Mask Ratio`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# home_scatter <- ggplot(df_figs, aes(x = `Home Ratio`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# diabetes_scatter <- ggplot(df_figs, aes(x = `% Adults with Diabetes`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# asthma_scatter <- ggplot(df_figs, aes(x = `% Deaths from Asthma`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# obesity_scatter <- ggplot(df_figs, aes(x = `% Adults with Obesity`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# stroke_scatter <- ggplot(df_figs, aes(x = `% Deaths from Stroke`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# heart_scatter <- ggplot(df_figs, aes(x = `% Deaths from Heart Disease`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# smoke_scatter <- ggplot(df_figs, aes(x = `% Adults who Smoke`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# respiratory_scatter <- ggplot(df_figs, aes(x = `% Deaths from Chronic
# Resp Disease`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# # unemployed_scatter <- ggplot(df_figs, aes(x = `% Unemployed`, y = `Fatality Rate`))+
# #   geom_point(size=2.5)
# # unemployed_scatter
# minority_scatter <- ggplot(df_figs, aes(x = `% Minority`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# crowding_scatter <- ggplot(df_figs, aes(x = `% Crowding`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# multiunit_scatter <- ggplot(df_figs, aes(x = `% Adults in Multi-
# Unit Dwellings`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# poverty_scatter <- ggplot(df_figs, aes(x = `% Adults in Poverty`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# elders_scatter <- ggplot(df_figs, aes(x = `% Adults >65`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# icubeds_scatter <- ggplot(df_figs, aes(x = `ICUBeds`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# icubedsper_scatter <- ggplot(df_figs, aes(x = `ICU Beds Per 10,000`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)
# income_scatter <- ggplot(df_figs, aes(x = `Household Income`, y = `Fatality Rate`))+ 
#   geom_point(size=2.5)

# test_scatter <- (mask_scatter | home_scatter)
# 
# test_scatter
# 
# test_scatter <- (diabetes_scatter | asthma_scatter)
# 
# test_scatter
# 
# test_scatter <- (obesity_scatter | stroke_scatter)
# 
# test_scatter
# 
# test_scatter <- (heart_scatter |  smoke_scatter)
# 
# test_scatter

# test_scatter <- (minority_scatter | crowding_scatter)
# 
# test_scatter
# 
# test_scatter <- (multiunit_scatter | poverty_scatter)
# 
# test_scatter
# 
# test_scatter <- (elders_scatter | income_scatter)
# 
# test_scatter
# 
# test_scatter <- (icubeds_scatter | icubedsper_scatter)
# 
# test_scatter
```


\textbf{Health}: For health variables, we observed that transformations did not improve the linear relationships between our explanatory variables and fatality rate. Therefore, no transformations were appropriate for any of the health variables. Overall, our diabetes, heart disease, and smoker features had positive correlations with fatality rate. Chronic respiratory disease and asthma both had negative correlations, which seemed counterintuitive, and obesity and stroke essentially had zero correlation.

\textbf{Policy}: For policy variables, no logarithmic or polynomial transformation conducted improved linear relationships between our explanatory variables and fatality rate.

\textbf{Socioeconomic}: We found that Percent of Minorities was the only socioeconomic feature that benefited from a logarithmic transformation. Figure 3 shows that applying a logarithmic transformation does make the best-fit curve slightly more linear. We use all other socioeconomic variables without transformations. We found that all socioeconomic variables had positive correlations with Fatality rate, except for Percent Crowding, which we did not expect since COVID-19 spreads faster when there is a higher concentration of people. Percent Unemployed and Percent Minority had the strongest positive correlations with Fatality Rate.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = 'A log transformation on \\% Minority improves its linear relationship with Fatality Rate, so we chose to apply the log in our final models.', fig.pos='!h', out.extra='', fig.align='center', out.width = "75%"}
#- show minority percentage scatterplots: normal vs logged --> Kevin
plot3 <- ggplot(df, aes(y = Fatality_Rate, x = Minority_Percentage)) +
 geom_point() + stat_smooth() + labs(x = '% Minority', y = 'Fatality Rate', title = 'Fatality Rate vs % Minority without Transformation') + theme_minimal()
plot4 <- ggplot(df, aes(y = Fatality_Rate, x = log(Minority_Percentage))) +
 geom_point() + stat_smooth() + labs(x = 'log(% Minority)', y = 'Fatality Rate', title = 'Fatality Rate vs % Minority with Transformation') + theme_minimal()
plot3 / plot4
```

\textbf{Miscellaneous}: ICU Beds per 10,000 does not require a transformation and can be used as-is.

Based on our collected variables and initial exploratory data analysis, we compiled subsets of our variables to include in our limited model, less limited model (Model 2), and most extensive model (Model 3). The following sections describe each model in detail, which features we chose, and some preliminary results.


<!--5. Are your choices supported by exploratory data analysis (*EDA*)? You will likely start with some general EDA to *detect anomalies* (missing values, top-coded variables, etc.). From then on, your EDA should be interspersed with your model building. Use visual tools to *guide* your decisions. You can also leverage statistical *tests* to help assess whether variables, or groups of variables, are improving model fit.
  - remove mask policy?-->
  
<!-- 5. Are your choices supported by exploratory data analysis (*EDA*)?

TODO: EDA for all 3 model types and reasoning for specific applied transformations
  - remove NA and check for reasoning with why they existed (make sure not all from one state, etc.)
  - scatter, hists, bar?, box 
  - variable summary, outlier removal? / reasoning
  - transformation logic (transformations to use,etc. include reasoning)
  
TODO: 
  - show minority percentage scatterplots: normal vs logged = Kevin
  - show scatterplots of Fatality_Rate vs everything else = Spencer Hong
  - show correlation of "before and after" = Jenny
  - TODO: Alaska Hawaii DC Boxplot Outlier Graphs = Spencer Song
  TODO: Try to label outliers ONLY ***
  
TODO: description of scatter below: From EDA, justification
This scatter plot is why we chose to use unemployment to represent our socioeconomic sectors.


  TODO: Blurb explaining the graphs below and how we chose to include this correlation plot and what it is used for. Connect to Model Building process. -->
  
<!-- ## Selecting Features for Models
  
Based on our collected variables and initial exploratory data analysis, we compiled subsets of our variables to include in our limited model, less limited model (Model 2), and most extensive model (Model 3). Our limited model includes three variables, one from each primary variable category we identified, pre-existing health conditions, policy, and socioeconomic factors. A discussion on how we chose our features for the limited model is discussed in the next section. Model 2 extends on our limited model to include several additional features, and Model 3 is the most inclusive model of all. To help us evaluate multicollinearity and causal relationships, we examined the correlations between each of explanatory variables for both Model 2 and Model 3. Figure 4 shows correlation matrices for Model 2, where the matrix on the left displays all features we were considering for our model and the right matrix shows the variables we ultimately decided to include in our model. The left matrix in Figure 4 shows that there is high correlation between several features, so we dropped % Deaths from Heart Disease and % Adults in Poverty to reduce multicollinearity. The right matrix shows the correlations after dropping these features, and there is great improvement. Figure 5 shows similar graphs for our Model 3. Because of high correlations between features, we drop Household Income, % Adults who Smoke, and % Adults with Obesity. This process helped us strategically select which features to include in our model to stay within the assumptions of the one-equation structural model, and our model results are described in the next section. -->


<!-- At the same time, it is important to remember that you are not trying to create one perfect model. You will create several specifications, giving the reader a sense of how robust (or sensitive) your results are to modeling choices, and to show that you're not just cherry-picking the specification that leads to the largest effects. -->

<!-- At a minimum, you should include the following three specifications:

1. **Limited Model**: A model that includes *only the key variable* you want to measure and nothing (or almost nothing) else. This variables might be transformed, as determined by your EDA, but the model should include the absolute minimum number of covariates (perhaps one, or at most two-three, covariates if they are so crucial that it would be unreasonable to omit them). 
1. **Model Two**: A model that includes *key explanatory variables and covariates that you believe advance your modeling* goals without introducing too much multicollinearity or causing other issues. This model should strike a balance between accuracy and parsimony and reflect your best understanding of the relationships among key variables.
1. **Model Three**: A model that includes the *previous covariates, and many other covariates*, erring on the side of inclusion. A key purpose of this model is to evaluate how parameters of interest change (if at all) when additional, potentially colinear variables are included in the model specification.-->
  


## Limited Model 

### Choosing Features for the Limited Model

Of all the variables we considered, we chose to include the duration a mask mandate policy was in place, the percent of adults diagnosed with diabetes, and the percent unemployment for each state. We chose one variable from each specific topic as to provide an encompassing view of possible causes for the change in fatality rates. These three were determined to be the most crucial from all the explanatory variables. For mask mandate, public health officials consistently stressed the efficacy to reduce transmission of COVID-19, which would reduce the number of fatalities. For percent adults with diabetes, we made the assumption that it encompasses several other health conditions as people with diabetes can tend to have additional underlying health conditions. Therefore, we wanted to include this variable as it would provide an overarching health explanatory variable. For percent unemployment, individuals without a job would have no or inadequate health care, which could lead them to having less accessibility and opportunities to receive medical treatments. During the pandemic, research shows that low-income areas have been severely impacted by COVID-19, which led to us choosing percent unemployment as the socioeconomic explanatory variable for the limited model.

### Model Equation

$\text{Fatality Rate} = \beta_0 + \beta_1 \text{ Mask Policy} + \beta_2 \text{ \% Adults with Diabetes} + \beta_3 \text{ \% Unemployment} + u$

```{r, echo = FALSE, warning = FALSE, message = FALSE}
model_limited <- lm(Fatality_Rate ~ mask_ratio + Diabetes_Diagnosed_Perc + 
                      Unemployment_Perc, 
                    data = df)
```

```{r excess code, echo = FALSE, warning = FALSE, message = FALSE}
#coeftest(model_limited)
#ols_vif_tol(model_limited)
#plot(density(resid(model_limited)))
#qqnorm(resid(model_limited))
#qqline(resid(model_limited))
```

### Limited Model Results

After running the coefficient test, we observe that both mask duration and unemployment percentage are both statistically significant at a 95% significance level or greater. However, mask duration has a positive impact on predicted fatality rates, which goes against our assumption that states that implemented mask mandates longer would observe a decrease in predicted fatality rates. Except, the model calculates, holding everything equal, with a one percent increase in the duration of a mask mandate, we would expect 0.003 percent increase in predicted fatality rates. For unemployment percentage, the model shows with a one percent increase in poverty, we would expect a 0.19 percent increase in predicted fatality rate.

The variance inflation factor (VIF) values for all three independent variables are all less than two, which indicates that there is minimal multicollinearity among them. With that said, the tolerance, or percent of variance in each variable that cannot be accounted for by the other predictors, of each variable is greater than 0.60. This suggests that there are variables not in the model that account for those predictors. Given this information, we construct a second model that includes a few more explanatory variables in order to lower the tolerance values. Then, we will compare Model Two to the Limited Model to determine which model provides a better fit for predicted values. 

## Model Two

### Choosing Variables for Model Two

For Model Two, we added to the variables included in our Limited Model. We initially considered several additional variables that did not make it into the final model. In particular, we decided to not include percent of deaths from heart disease and percent of adults living in poverty because of their strong correlations with other features. To help us evaluate multicollinearity and causal relationships, we examined the correlations between each of explanatory variables. Figure 4 shows correlation matrices for Model Two, where the matrix on the left displays all features we were considering for our model and the right matrix shows the variables we ultimately decided to include in our model. The left matrix in Figure 4 shows that there is high correlation between several features, so we dropped % Deaths from Heart Disease and % Adults in Poverty to reduce multicollinearity. The right matrix shows the correlations after dropping these features, and there is great improvement. The final new variables included in Model Two are the duration of stay-at-home policies, percentage of deaths from chronic respiratory diseases, and the log-transformation of minority percentage of each state, in addition to the features already in the Limited Model.

```{r, echo=FALSE, warning = FALSE, message = FALSE, fig.cap="Changes in correlations between explanatory variables before (left) and after (right) removing some features in Model 2.", fig.pos='!h', out.extra='trim={0 .5cm 0 0},clip', fig.align="center"}

vars_2_reduced <- subset(df_figs, select = c('Mask Ratio', "% Adults with Diabetes", '% Unemployed', 'Home Ratio', '% Deaths from Chronic
Resp Disease', '% Minority'))

vars_2_orig <- subset(df_figs, select = c('% Deaths from Heart Disease', '% Adults in Poverty', 'Mask Ratio', "% Adults with Diabetes", '% Unemployed', 'Home Ratio', '% Minority', '% Deaths from Chronic
Resp Disease'))

res_2_orig <- cor(vars_2_orig)
model_2_plot1 <- ggcorrplot(res_2_orig, type = "upper", method = "square", tl.cex = 8, tl.srt = 90, hc.order = FALSE, colors = c("#6D9EC1", "white", "#E46726"), show.legend = FALSE) 

res_2_reduced <- cor(vars_2_reduced)
model_2_plot2 <- ggcorrplot(res_2_reduced, type = "upper", method = "square", tl.cex = 8, tl.srt = 90, show.legend = FALSE, colors = c("#6D9EC1", "white", "#E46726"))

model_2_combined_plot <- (model_2_plot1 | model_2_plot2) + theme(legend.position='bottom')

model_2_combined_plot
```

### Model Equation

$\text{Fatality Rate} = \beta_0 + \beta_1 \text{ Mask Policy} + \beta_2 \text{ \% Adults with Diabetes} + \beta_3 \text{ \% Unemployment} + \beta_4 \text{ Home Policy} + \beta_5 \text{ \% Chronic Resp Deaths} + \beta_6 \text{ log(\% Minority)} + u$

```{r, echo = FALSE, message = FALSE, warning = FALSE}
model_two <- lm(Fatality_Rate ~ mask_ratio + Diabetes_Diagnosed_Perc + 
                  Unemployment_Perc + home_ratio + 
                  Chronic_Respiratory_Disease_Death_Perc + log(Minority_Percentage), 
                data = df)
```

```{r more extra code, echo = FALSE, message = FALSE, warning = FALSE}
#coeftest(model_two)
#ols_vif_tol(model_two)
#plot(density(resid(model_two)))
#qqnorm(resid(model_two))
#qqline(resid(model_two))

#print(anova(model_limited, model_two))
```

### Model Two Results

With the additional variables, Model Two has three statistically significant explanatory variables at a 90% or greater level of significance: percent of adults diagnosed with diabetes, percent unemployment, and percent of deaths from chronic respiratory disease. Also, we observe that the mask mandate policy variable is no longer statistically significant. Both percent of adults diagnosed with diabetes and unemployment percentage have a positive effect on fatality rates while the percent of chronic respiratory deaths has negative effect, which is concerning as we would expect to have an increased effect on fatality rates as percent of chronic respiratory deaths increases.

The VIF values for all six independent variables are all less than four, which indicates that there is minimal multicollinearity among them. In addition, the tolerance levels of each variable were lower than the Limited Model indicating that the added variables accounted for more for one another than the previous model. The Anova test we ran to compare the fits of the Limited Model and Model Two calculates a p-value less than a 0.05 level of significance level. This shows that by adding the additional variables leads to a more significant improvement of predicted fatality rates compared to the Limited Model. We will proceed to constructing a third model that will include several additional features, and Model Three will similarly be examined if it is a better fit than Model Two as currently Model Two is our preferred model.

## Model Three

### Choosing Variables for Model Three

Model Three is our most extensive model and includes six additional explanatory variables: percent of adults living in multi-unit dwellings, percent of crowding, percent of asthma deaths, ICU beds per 10,000, percent of stroke deaths, and percent of adults over the age of 65. When choosing which features to include in Model Three, we performed a similar multicollinearity analysis that we did for Model Two. The left correlation matrix in Figure 5 shows correlations between all variables we were initially considering. Because of high correlations between features, we dropped Household Income, % Adults who Smoke, and % Adults with Obesity. This process helped us strategically select which features to include in our model to stay within the assumptions of the one-equation structural model, and our model results are described in the next section.

```{r, echo=FALSE, warning = FALSE, message = FALSE, fig.cap='Changes in correlations between explanatory variables before (left) and after (right) removing some features in Model 3.', fig.pos='!h', out.extra='trim={0 1cm 0 0},clip', fig.align="center"}

vars_3_orig <- subset(df_figs, select = -c(`State`, `Deaths`, `Cases`, `Population`, `Population..20..`, `Population >65`, `ICUBeds`, `Fatality Rate`, `% Adults in Poverty`, `% Deaths from Heart Disease`))

vars_3_reduced <- subset(df_figs, select = -c(`State`, `Deaths`, `Cases`, `Population`, `Population..20..`, `Population >65`, `ICUBeds`, `Fatality Rate`, `% Adults in Poverty`, `% Deaths from Heart Disease`, `Household Income`, `% Adults who Smoke`, `% Adults with Obesity`))

res_3_orig <- cor(vars_3_orig)
model_3_plot1 <- ggcorrplot(res_3_orig, type = "upper", method = "square", tl.cex = 6, tl.srt = 90, hc.order = FALSE, colors = c("#6D9EC1", "white", "#E46726"), show.legend = FALSE) 

res_3_reduced <- cor(vars_3_reduced)
model_3_plot2 <- ggcorrplot(res_3_reduced, type = "upper", method = "square", tl.cex = 6, tl.srt = 90, show.legend = FALSE, colors = c("#6D9EC1", "white", "#E46726"))

model_3_combined_plot <- (model_3_plot1 | model_3_plot2) + theme(legend.position='bottom')

model_3_combined_plot
```

### Model Equation 

$\text{Fatality Rate} = \beta_0 + \beta_1 \text{ Mask Policy} + \beta_2 \text{ \% Adults with Diabetes} + \beta_3 \text{ \% Unemployment} + \beta_4 \text{ Home Policy} + \beta_5 \text{ \% Chronic Resp Deaths} + \beta_6 \text{ log(\% Minority)} + \beta_7 \text{ \% Multi-Unit Dwelling)} + \beta_8 \text{ \% Crowding} + \beta_9 \text{ \% Asthma Deaths} + \beta_{10} \text{ \% ICUBedsPer10K} + \beta_{11} \text{ \% Stroke Deaths} + \beta_{12} \text{ \% Adults >65} + u$

```{r, echo = FALSE, message = FALSE, warning = FALSE}
model_three <- lm(Fatality_Rate ~ Multi_Unit_Perc + Unemployment_Perc + home_ratio + 
                    log(Minority_Percentage) + Crowding_Perc + Asthma_Deaths_Perc + 
                    Chronic_Respiratory_Disease_Death_Perc + ICUBedsPer10000 + 
                    Diabetes_Diagnosed_Perc + Stroke_Death_Perc + mask_ratio + Senior_Perc, 
                  data = df)
```

```{r even more extra code, echo = FALSE, message = FALSE, warning = FALSE}
#coeftest(model_three)
#ols_vif_tol(model_three)
#plot(density(resid(model_three)))
#qqnorm(resid(model_three))
#qqline(resid(model_three))

#print(anova(model_two, model_three))
```


### Model Three Results

With the additional variables, Model Three has three statistically significant explanatory variables at a 95% or greater level of significance: unemployment percentage, percent of adults diagnosed with diabetes, and percent of stroke deaths. Both unemployment percentage and percent of adults diagnosed with diabetes have a positive effect on fatality rates while the percent of stroke deaths has negative effect, which is odd as we would expect to have an increased effect on fatality rates as percent of stroke deaths increases.

Other than the predictor variable for percent of adults diagnosed with diabetes having a VIF value larger than five, all other VIF values are less than five suggesting that there is minimal multicollinearity. However, this is the first model to show signs of multicollinearity, which means that we should look to not validate this model as our preferred as the multicollinearity present could reduce the precision of the predictor variables and that we might not be able to have high confidence in the p-values for each variable and its statistical significance. The Anova test for Model Two versus Model Three calculates a p-value less than 0.05, which would designate Model Three is a better fit model than Model Two. 

## Comparing and Selecting Our Model

Comparing the three models above, we have chosen to emphasize Model Two. First, Model Two includes more explanatory features than the Limited Model and Model Two has proven to be a better fit than the Limited Model via the Anova test we ran. Model Three does appear to be statistically a better fit than Model Two; however, we have concerns about causality and the presence of multicollinearity. Additionally, the large negative coefficient for the percent of stroke deaths predictor variable in Model Three is worrying. Therefore, we have chosen Model Two as our preferred model for the rest of our analysis.

<!--$\text{Fatality Rate} = \beta_0 + \beta_1 \text{ Mask Policy} + \beta_2 \text{ % Adults with Diabetes} + \beta_3 \text{ % Unemployment} + \beta_4 \text{ Home Policy} + \beta_5 \text{ % Chronic Resp Deaths} + \beta_6 \text{ log(% Minority)} + u$-->


<!-- Although the models have different emphases, each one must still be a reasonable choice given your modeling goals.  The idea is to choose models that encircle the space of reasonable modeling choices, and to give an overall understanding of how these choices impact results. -->

# Regression Table, Interpreting Model Coefficients, and Practical Significance

<!-- You should display all of your model specifications in a regression table, using a package like [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) to format your output. It should be easy for the reader to find the coefficients that represent key effects at the top of the regression table, and scan horizontally to see how they change from specification to specification. Make sure that you display the most appropriate standard errors in your table, along with significance stars.

In your text, comment on both *statistical significance and practical significance*. You may want to include statistical tests besides the standard t-tests for regression coefficients. -->

We group the coefficients and results of our models into a stargazer table to facilitate analysis.  We will focus in particular on model two since it is the model that provides a better fit but does not present high multicollinearity among its variables as in the third model.  We also chose to use the VIF table over the scatterplot matrix due to the fact that many of the variables have already been pre-categorized and have been found to have high collinearity from our initial exploratory data analysis.  Thus we believe assessing the VIF table will help us better analyze the possibility of a relationship between independent variables and gain further insight into the effects of multicollinearity.

```{r get robust ses, echo = FALSE, message = FALSE, warning = FALSE}
rse <- function(model) { 
  sqrt(diag(vcovHC(model)))
  }
```

```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
sg_table <- stargazer(
  model_limited, model_two, model_three,
  se = list(rse(model_limited), rse(model_two), rse(model_three)),
  omit.stat = 'f',
  header = FALSE,
  title = 'Coefficients and Significant Levels for All Models'
  )
# sg_table
```
 
```{r, echo = FALSE, message = FALSE, warning = FALSE, results = 'asis'}
VIF <- left_join(left_join(ols_vif_tol(model_three), ols_vif_tol(model_two), by = c('Variables')),ols_vif_tol(model_limited), by = c('Variables')) %>% 
  select(1,7,5,3) %>% 
  rename('Model Limited' = VIF,
         'Model Two' = VIF.y,
         'Model Three' = VIF.x
         )
VIF[,c(2,3,4)] <- round(VIF[,c(2,3,4)], digits=5)
VIF[is.na(VIF)] <- ''
stargazer(VIF,
          summary = FALSE, 
          title = 'Variance Inflation Factors for All Three Models',
          header = FALSE,
          rownames = FALSE)
```


## Interpreting Statistical and Practical Significance

### Limited Model

In the Limited Model, Table 1 shows that the mask_ratio variable is statistically significant with a p value less than 0.05. The model also returns unemployment percent as statistically significant. With respect to practical significance, for every percent of the time the mask policy was enacted, all else held constant, we expect to see a 0.3% increase in fatality rates.  Which is counter intuitive considering we were expecting the mask policy to contribute to lowering COVID-19 deaths. Another variable that was tested to be statistically significant is the unemployment percentage. The coefficient of 0.195 implies that without any effects from other variables, a one percent increase in unemployment effectively increases the fatality rate by 19.5% thus pointing to a large practical significance.  
  
### Model Two

In the second model, unemployment percent and chronic respiratory disease death percent are shown to be statistically significant with a p value less than 0.01. 

With regards to practical significance, we see an increase in the coefficient for unemployment percent in the second model resulting in a stronger practical significance the variable would have on the fatality rate. For every increase in percentage in pre-pandemic unemployment rate we would look to expect a increase of 21% in fatality rate.  Outside of the model, an underlying issue associated with higher unemployment is the fact that the worker no longer has income and health insurance leaving the state more vulnerable when faced with a health related crisis. Therefore it would be safe to conclude that unemployment rate within each state does have practical significance in this context.

Continuing with practical significance, we see that chronic respiratory disease death percent has an abnormally large coefficient where we can conclude that for every percent change in the death rate for chronic respiratory disease we expect to see a 2,172% decrease in fatality rate. While this number seems high, this is likely because the death rate from chronic respiratory disease is a small number---with the average being 0.057%---so a 1% increase in chronic respiratory disease deaths would be 17 times increase in the existing rate. In a national study done by the CDC, chronic respiratory diseases is ranked 4th in leading causes of death in the US [(CDC)](https://wonder.cdc.gov/controller/datarequest/D76;jsessionid=90C1B29EB24599F0FE3984D4DF2C). On top of the large coefficient in the model, it is clear that chronic respiratory disease death percent has large practical significance as it poses a potential risk affecting up to 5% of the U.S. adult population. 

The percent diagnosed with diabetes is shown to be statistically significant but less so with a p-value less than 0.1. We can infer from the results that for every percent increase in the population that is diagnosed with diabetes, we see a 9.6% increase of fatality rate in the model. In another study done by the CDC, they estimate a total of 13% of the US population has some form of diabetes [(CDC)](https://www.cdc.gov/diabetes/pdfs/data/statistics/national-diabetes-statistics-report.pdf), while the percent effect the variable has on fatality rate is smaller compared to chronic respiratory disease death rates, the sheer percentage of the population that this would apply to makes this variable practically significant.

Finally, we do see mask ratio lose its statistical significance while still maintaining a similar level of practical significance to our Limited Model. However, logically speaking, the purpose of the mask policy is to prevent the user from catching or transmitting COVID-19, and since our model is looking to gauge the effect it has on fatality rates, we would not consider the mask ratio variable to be practically significant.  

### Model Three

In the third model, we see unemployment percent and percent diagnosed with diabetes continue to be statistically significant with p values less than 0.01 and 0.05, respectively. We are also met with stroke death percent as another variable that is observed to be highly statistically significant. The affect of stroke death percent is shown to have the highest practical significance, with an affect of 4,387% decrease in fatality rate for every percent increase, when compared to any other health variable measured on the same scale. The chronic respiratory disease death rate loses both its statistical significance as well as some of its practical significance with the inclusion of the additional health variables. Finally, it is important to note that the increased statistical significance seen in the variable representing the percent of population diagnosed with diabetes also came with an increase to the practical significance with a factor of two resulting in a 18.4% increase in fatality rate for every increase in percent of the state population diagnosed with diabetes.

Comparing the three models, it is clear that Model Two does the best job of explaining COVID-19 fatality rates. The coefficients in unemployment percent is similar to that of the first model and with the inclusion of other variables such as chronic respiratory disease death percent allows us to improve our model by 28%. Comparing the models on the VIF chart in Table 2, we can see a dramatic increase in VIF values jumping from Model Two to Model Three indicating high multicollinearity among the variables that originally existed within model two. Several variables are propelled to values higher than 4 with diabetes percent crossing over the threshold to 5.35. This means that adding the additional variables into the third model did not dramatically improve our model and might have caused underlying factors to be represented in multiple variables. Looking at the standard errors for each coefficient, asthma deaths percent stands out as having an exceedingly high standard error of 302.008 with a coefficient of -31.038. Chronic respiratory disease death percent also shows high mullticollinearity in the third model with a reduction in practical significance but a dramatic increase in standard error.

<!--How Models 1 and 3 differed from Model 2. How it impacted our interpretations?-->


# Limitations on Model

<!-- As a team, evaluate all of the CLM assumptions that must hold for your model. However, do not report an exhaustive examination all 5 CLM assumption. Instead, bring forward only those assumptions that you think pose significant problems for your analysis. For each problem that you identify, describe the statistical consequences. If you are able to identify any strategies to mitigate the consequences, explain these strategies. 

Note that you may need to change your model specifications in response to violations of the CLM. 

TODO: CLM Assumptions and reasoning for violations
  TODO: how those violations should change our model specifications
TODO: strategies to mitigate violations

TODO: causality concerns / collinearity
  - check plots of var v. var
  - correlation matrix
  - draw causal theories?

- IID -->

We assess the five classical linear model (CLM) assumptions and the violations of the one-equation structural model for our Model 2. This causal analysis relies on the CLM because our dataset consists of only 48 observations, one for each state in the contiguous U.S. Because of this, an accurate interpretation of our results requires analyzing the CLM assumptions: independent and identically distributed random variables (IID), linear conditional expectation, no perfect collinearity, homoskedastic errors, and normally distributed errors. Additionally, we assess if our causal analysis violates any assumptions of the one-equation structural model, including omitted variables, reverse causality, and causal relationships between explanatory variables.

Our model met the CLM assumptions well. There is some concern about our data being IID because of geographical effects. In particular, for socioeconomic factors and health conditions, some regions of the country suffer more than others. For instance, diabetes is much more prominent in the southern states than elsewhere in the country. We must proceed with caution and assume that our data is satisfactory to meet the IID condition to continue with our analysis. For the linear conditional expectation assumption, we observed that our model tends to slightly underpredict states with particularly low or high fatality rates and overpredict states with mid-range fatality rates. As shown in Figure 6, our residuals versus predictions plot was not exactly linear around zero, which implies our linear model may not be the best fit. Our results are shown in Figure 6. However, applying additional logarithmic transformations did not significantly improve the residuals, so we left our model with only % Minority having a log transformation. We proceed with caution when interpreting the model coefficients. A nonlinear model may have fit the data better, but this is out of scope for our work.

```{r, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = 'The residuals versus predictions scatterplot for Model Two shows that our model does not perfectly meet the linear conditional expectation requirement since our model overpredicts at the edges and underpredicts in the middle.', fig.pos='!h', out.extra='', fig.align="center", out.width = "75%"}
df %>% 
  mutate(
    model_two_preds = predict(model_two), 
    model_two_resids = resid(model_two)
  ) %>% 
  ggplot(aes(model_two_preds, model_two_resids)) + 
  geom_point() + labs(x = 'Model Two Predictions', y = 'Model Two Residuals') + 
  stat_smooth() + ggtitle("Model Two: Residuals vs. Predictions") + theme_minimal()
```

## Violations of One-Equation Structural Model

We believe there are several concerns with our model and the assumptions of the one-equation structural model. First, there could plausibly exist some reverse causality in our model, particularly with the policy variables. For example, our model currently assumes that longer mask mandates will impact COVID-19 fatality rates. However, it is also reasonable to say that high COVID-19 fatality rates will cause mask mandates to last longer. The latter causality statement is likely stronger than the first, which is why our coefficient on Mask Ratio is positive. The coefficient on this feature, therefore, explains the correlation between Mask Ratio and Fatality rate but not the causality. Second, it is also plausible that causal relationships exist between our explanatory variables. This is likely most prevalent among our health features. We tried to account for this by limiting the number of health variables we use. For instance, we omitted % of Adults with Obesity but left % of Adults with Diabetes in our model because obesity is a direct cause of diabetes. However, it is likely that we did not catch all of these relationships, which causes some causal concerns with our health features. Finally, omitted variable bias is another concern, which is discussed further in the next section.


<!-- Our comments
- edge effects not lookin too good

###> This doesn't look good. There seems to be a pretty clear non-linear relationship in this data -- perhaps parabolic or logarithmic.
###> To correct this, we will try variable transformations, hoping to end up with a more linear pattern in this plot.
### > Additionally, you may sometimes want to create predictor versus fitted plots, which may give you clues about what variables you need to transform.-->
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# vars_2_reduced <- subset(df_figs, select = c('Mask Ratio', "% Adults with Diabetes", '% Unemployed', 'Home Ratio', '% Deaths from Chronic
# Resp Disease', '% Minority'))

# df <- df %>% 
#   mutate(
#     model_two_preds = predict(model_two), 
#     model_two_resids = resid(model_two)
#   ) 
# mask_resids <- df %>% 
#   ggplot(aes(mask_ratio, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
# diabetes_resids <- df %>% 
#   ggplot(aes(Diabetes_Diagnosed_Perc, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
# unemployment_resids <- df %>% 
#   ggplot(aes(Unemployment_Perc, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
# home_resids <- df %>% 
#   ggplot(aes(home_ratio, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
# resp_resids <- df %>% 
#   ggplot(aes(Chronic_Respiratory_Disease_Death_Perc, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
# minority_resids <- df %>% 
#   ggplot(aes(Minority_Percentage, model_two_resids)) + 
#   geom_point() + 
#   stat_smooth()
```

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# mask_resids
# diabetes_resids
# unemployment_resids
# home_resids
# resp_resids
# minority_resids
```
<!-- - looks a bit worse, edge effects still


- No perfect collinearity
### > First, we can look at our coefficients, and notice that R has not dropped any variables.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
# model_two$coefficients
```
- no perfect collinearity

```{r, message = FALSE}
# df %>% 
#   select(
#     Fatality_Rate, mask_ratio, Diabetes_Diagnosed_Perc, Poverty_Perc, home_ratio, Chronic_Respiratory_Disease_Death_Perc, Minority_Percentage, model_two_resids
#   ) %>%
#   GGally::ggpairs()
```


### > This tells us that there is no perfect collinearity.  This assumption also includes the requirement that a BLP exists, which may not happen if there are heavy tails.  In this case, though, we don't see any distributions that look like they have unusually low or high values.
- Homoskedastic errors
### > To assess whether the distribution of the errors is homoskedastic, we can examine the residuals versus fitted plot again.  We are interested in whether there is a band of even thickness from left to right. Looking at the plot above, indeed, it does look like there might be some increase in the variance of the residuals at the upper side of the predicted values, but it is not severe.  We may hope that a log transform fixes this small problem
### > Another idea is to examine the scale-location plot. Homoskedasticity would show up on this plot as a flat smoothing curve.
 
```{r}
# plot(model_two, which=3)
```
- homoskedasticity actually not lookin too bad = model_two looking 10/10!!!

### > This plot looks quite good, suggesting no major problem with heteroskedasticity.
- Normally distributed errors
```{r}
# plot_one <- df %>% 
#   ggplot(aes(x = model_two_resids)) + 
#   geom_histogram()
#   
# plot_two <- df %>% 
#   ggplot(aes(sample = model_two_resids)) + 
#   stat_qq() + stat_qq_line()
# plot_one / plot_two
```
- not too bad!! some minor deviation from normality towards the tails
- high frequency of values just below -0.005

> The histogram of residuals and the qqplot shows some some deviation from normality, specifically a right skew and perhaps an unusual concentration on the right tail.
>
> This is not a problem for unbiasedness, and it is not a problem for our standard errors.  However, this will threaten the validity of our t-tests and confidence intervals.  We may hope to fix this problem with a variable transformatin. -->


# Discussion of Omitted Variables

Because we are assessing a causal question, we now need to check for omitted variable bias. We want to apply OVB concepts to our second regression model.

## Model Equation

$\text{Fatality Rate} = \beta_0 + \beta_1 \text{ Mask Policy} + \beta_2 \text{ \% Adults with Diabetes} + \beta_3 \text{ \% Unemployment} + \beta_4 \text{ Home Policy} + \beta_5 \text{ \% Chronic Resp Deaths} + \beta_6 \text{ log(\% Minority)} + u$

## Omitted Variables

We have determined the following as our 5 most important omitted variables:

1. \textbf{Food Quality}: Quality and availability of healthy food throughout a state.
Food Quality throughout a state would likely be negatively correlated with COVID-19 fatality cases. Overall, a healthier population will likely be more adverse to higher fatality rates and severe reactions to COVID-19.
Food Quality will likely have no correlation with state mask policies, stay-at-home policies, and chronic respiratory disease percentages. We observe no OVB for these independent variables.
Food Quality will likely have a high negative correlation with diabetes diagnoses and a negative correlation with unemployment percentages. Because the independent variables have a positive impact on fatality rates and have a negative correlation with our omitted variable, we conclude that the direction of the OVB is towards zero, as we have likely overstated the effects of these independent variables on fatality rates.
Food Quality will likely have a negative correlation with minority percentages, and as minority has a negative causal impact on fatality rates, the direction of the OVB will be away from zero, and we have overstated the impact of minority percentage.
There may not necessarily be a good replacement for this variable in the data that we have gathered, as it is categorically very different from the rest of our other explanatory variables.

2. \textbf{Perception of COVID-19 Severity:} A survey value of the average perception of how severe / fatal COVID-19 is.
We believe that a higher perception of how severe / fatal COVID-19 would correlate with lower fatality rates. People who are more concerned about the severity of COVID-19 will likely be more cautious in caring for their own health and isolate if they have underlying health conditions or are immunocompromised.
Perception of COVID-19 Severity will likely have no correlation with diabetes diagnoses, unemployment percentages, and minority percentages. Perception of COVID-19 Severity will likely have a strong positive correlation with state mask policies, a positive correlation with state stay at home policies, and a positive correlation with chronic respiratory disease percentages. As state mask policies have a positive causal impact on fatality rates, our direction of our OVB is towards zero. But for stay-at-home policies and chronic respiratory disease percentages, we have a negative causal impact on fatality rates, so the direction fo our OVB is away from zero. For these policies, we are likely overstating their impact compared to a regression where we include Perception of COVID-19 Severity.
We dont necessarily have a great replacement for this variable. We would have assumed that policies would be a replacement, but we have inverse relationships in both of our policies, so it isnt clear how we could find a replacement for this variable.

3. \textbf{\% registered Republican:} A higher percentage of Republican voters in a state.
We believe that % registered Republican will have a negative correlation with fatality rates. We believe that higher Republican percentages will be correlated with less COVID-19 resources and policies in general from state government.
% registered Republican will likely have no correlation with diabetes percentages and chronic respiratory percentages. This variable will also have a strong negative correlation with state mask policies and stay-at-home policies. This shows that the direction of our bias is away from zero.
This variable will likely have a strong negative correlation with minority percentages and unemployment rates, which will direct our bias further away from zero.
This is also a difficult line of reasoning, as we have seen from our regression that policies in general led to a decrease in fatality rates, which suggests some lines of reverse causality from what we originally thought. So the bias for this variable could very easily be switched and be towards zero instead. Our policy variables are a good replacement.

4. \textbf{Population \% in Urban Zones:} The percentage of the states population that live in urban zones.
We believe that this variable will have a positive correlation with fatality rates, as clusters of individuals would likely be more quickly impacted by COVID-19.
This variable likely has no correlation with ethnic percentages, unemployment percentages, and diabetes percentages.
This variable likely has a positive correlation with ethnic percentages, as urban zones correlate with higher minority rates. This would show that here we have a bias that is towards zero. 
This variable likely has a positive correlation with chronic respiratory diseases---if more people live in urban zones, there are more people exposed to local air pollution, so there are also likely more individuals suffering from chronic respiratory diseases. The direction of this bias would also be towards zero because chronic respiratory disease deaths has a negative causal impact on fatality rate.
On the other hand, this variable is likely to have a positive correlation with mask policy and stay-at-home populations. Higher populations in urban zones will likely correlate with more necessary regulations across a state for COVID-19. These relationships show that our policy variables have an OVB that is away from zero for mask policies and towards zero for stay-at-home policies. We do not have a variable in our model that can represent this statistic, as crowding is focused more on the specifics of urban populations.

5. \textbf{Exercise Rates:} Mean exercise per person per week.
We believe that Exercise Rates will be negatively correlated with fatality rates. Healthier populations should, in general, be correlated with lower fatality rates. 
Exercise Rates will likely have no correlation with state mask policies, stay-at-home policies, minority percentages, and unemployment percentages. This omitted variable will likely have a negative correlation with diabetes and chronic respiratory diseases, which shows that our direction of our bias is away from zero for chronic respiratory disease and towards zero for diabetes. We have likely been fair in our statement of the causal effect of health variables on fatality rates, but there may be effect. We dont necessarily have a variable that can replace activity in a population, but our health variables are likely indicators that would impact fatality rates.

```{r, echo = FALSE, message = FALSE, warning = FALSE}
# ovb_table <- data.frame("Independent Variable Names" = c("Mask Policy", "Diabetes %", "Unemployment", "Stay-at-Home Policy", "Chronic Respiratory Disease %", "Minority"), "Food Quality" = c(), "" = c("John","Dora"))
#stargazer()
```

Given our findings using our ANOVA testing and modeling, we have seen that our independent variables are in general directed away from zero. We are likely artificially introducing some significance in our model through omitted variable bias. Considering our model and significance results, the primary covariate of concern is Percent of Adults Diagnosed with Diabetes. Percent diabetes was found to be significant but at the lowest significance level with a p-value of 0.094. A correct model would most likely not find that diabetes is statistically or practically significant.

# Conclusion
<!-- Make sure that you end your report with a discussion that distills key insights from your estimates and addresses your research question.


- Our expectations and assumptions going into EDA
- A deeper dive into practical significance and what we took away from this and how it relates to our research question
- Unexpected relationships and possible logic paths for explanation, roadblocks
- Why we should have done this shit on a county level

TODO: all-->

Our goal was to analyze what causes varying COVID-19 fatality rates across states. By collecting data on implemented policy, pre-existing health conditions, and socioeconomic factors, we built a one-equation structural model for this causal question. We analyzed three different models and concluded that our Model Two was most successful, since it fit our data better than our Limited Model but met the one-equation structural model assumptions better than Model Three. From Model Two, we can conclude that Percent of Chronic Respiratory Disease Deaths, Percent Unemployment, and Percent of Adults Diagnosed with Diabetes are the most significant features that cause variation in COVID-19 fatality rates. Our model indicated that higher rates of unemployment and diabetes cause higher fatality rates, but higher rates of chronic respiratory disease cause lower fatality rates. The latter of these observations seemed counterintuitive to us, which caused us to question if our model met all assumptions for both the Classical Linear Model and the one-equation structural model.

While Model Two met the CLM assumptions relatively well, we believe our model faced several violations for the one-equation structural model, including reverse causality, causal relationships among the explanatory variables, and omitted variable bias. Although we tried to strategically choose which variables we included in Model Two by looking at correlation matrices, we believe we did not fully mitigate issues with reverse causality and causal relationships among the explanatory variables. There were also several omitted variables that likely impact the coefficients produced in our model. Based on our OVB analysis, we concluded that diabetes may not actually be statistically significant.

Due to concerns of one-equation structural model violations, we believe our coefficients are untrustworthy. Because of this, we recommend a future study analyzing COVID-19 fatality rates on a county-level instead of a state-level. Within each state, there is significant variation for each covariate we analyzed. Performing this analysis at a state-level resulted in a loss of this detailed variation, and it is likely that the aggregated data played a role in our poor model results. Additionally, a county-level analysis would include more observations, so each individual data point would have less influence over model coefficients. Our current Model Two was highly susceptible to the values of each observation, which likely resulted in our high coefficients of 2,172% and 4,387% mentioned in our Interpreting Statistical and Practical Significance Section. In future work, we should also work to fully eliminate reverse causality, ensure explanatory variables are truly independent, and incorporate more variables to avoid OVB. 

<!--## Rubric for Evaluation

You may use the following, loosely structured rubric to guide your writing.

- **Introduction.** Is the introduction clear? Is the research question specific and well defined? Does the introduction motivate a specific concept to be measured and explain how it will be operationalized. Does it do a good job of preparing the reader to understand the model specifications?

- **The Initial Data Loading and Cleaning.** Did the team notice any anomalous values? Is there a sufficient justification for any data points that are removed? Did the report note any coding features that affect the meaning of variables (e.g. top-coding or bottom-coding)? Overall, does the report demonstrate a thorough understanding of the data? Does the report convey this understand to its reader -- can the reader, through reading this report, come to the same understanding that the team has come to? 

- **The Model Building Process.** Overall, is each step in the model building process supported by EDA? Is the outcome variable appropriate? Did the team clearly state why they chose these explanatory variables, does this explanation make sense in term of their research question? Did the team consider available variable transformations and select them with an eye towards model plausibility and interpretability? Are transformations used to expose linear relationships in scatterplots? Is there enough explanation in the text to understand the meaning of each visualization?

- **Regression Models:**
   - **Base Model.** Does this model only include key explanatory variables? Do the variables make sense given the measurement goals? Did the team apply reasonable transformations to these variables, to capture the nature of the relationships? Does the team write about this model in prose in a way that is appropriate? 
   - **Second Model.** Does this model represent a balanced approach, including variables that advance modeling goals without causing major issues? Does the model succeed in reducing standard errors of the key variables compared to the base model? Does it capture major non-linearities in the joint distribution of the variables? Does the team write about this model in prose in a way that is appropriate? 
   - **Third Model.** Does this model represent a maximalist approach, erring on the side of including most variables? Is it still a reasonable model? Are there any variables that are outcomes, and should therefore still be excluded? Is there too much colinearity, to the point that the key causal effects cannot be measured? Does this team write about this model in prose in a way that is appropriate? 

- **A Regression Table.** Are the model specifications properly chosen to outline the boundary of reasonable choices? Is it easy to find key coefficients in the regression table? Does the text include a discussion of practical significance for key effects? 
   
- **Plots, Figures, and Tables** Do the plots, figures and tables that the team has chosen to include successfully move forward the argument that they are making? Has the team chosen the most effective method (a table or a chart) to display their evidence? Is that table or chart the most communicative it could be? Is every plot, figure, and table that is included in the report referenced in the narrative argument?

- **Assessment of the CLM.** Has the team presented a sober assessment of the CLM assumptions that might be problematic for their model? Have they presented their analysis about the consequences of these problems (including random sampling) for the models they estimate? Did they use visual tools or statistical tests, as appropriate? Did they respond appropriately to any violations?

- **An Omitted Variables Discussion.** Did the report miss any important sources of omitted variable bias? Are the estimated directions of bias correct? Was their explanation clear? Is the discussion connected to whether the key effects are real or whether they may be solely an artifact of omitted variable bias?

- **Conclusion.** Does the conclusion address the research question? Does it raise interesting points beyond numerical estimates? Does it place relevant context around the results?

- Are there any other errors, faulty logic, unclear or unpersuasive writing, or other elements that leave you less convinced by the conclusions? -->
